# 注意力就是你所需要的一切

# 摘要

主流的序列转换模型基于复杂的循环或卷积神经网络，包括一个编码器和一个解码器。性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，完全基于注意力机制，完全摒弃了循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优越，同时具有更好的并行性，训练时间显著减少。我们的模型在WMT 2014英德翻译任务上达到了28.4 BLEU分数，比现有最佳结果（包括集成模型）提高了超过2个BLEU分数。在WMT 2014英法翻译任务上，我们的模型在8个GPU上训练3.5天后建立了新的单模型最先进BLEU分数41.8，这只是文献中最佳模型训练成本的一小部分。我们通过将Transformer成功应用于英语成分句法分析（在大型和有限训练数据下），展示了Transformer能够很好地泛化到其他任务。

# 1 引言

循环神经网络，特别是长短期记忆网络[13]和门控循环神经网络[7]，已经被牢固确立为序列建模和转换问题（如语言建模和机器翻译[35, 2, 5]）的最先进方法。此后，众多研究持续推进循环语言模型和编码器-解码器架构的边界[38, 24, 15]。

循环模型通常沿着输入和输出序列的符号位置进行计算分解。通过将位置与计算时间步骤对齐，它们生成隐藏状态序列$h_t$，作为前一个隐藏状态$h_{t-1}$和位置$t$的输入的函数。这种内在的序列性质阻碍了训练样本内的并行化，在较长序列长度时变得关键，因为内存约束限制了样本间的批处理。最近的工作通过分解技巧[21]和条件计算[32]在计算效率方面取得了显著改进，后者还提高了模型性能。然而，序列计算的基本约束仍然存在。

注意力机制已成为各种任务中引人注目的序列建模和转换模型的组成部分，允许对依赖关系进行建模而无需考虑它们在输入或输出序列中的距离[2, 19]。然而，除了少数情况[27]外，这种注意力机制都与循环网络结合使用。

在这项工作中，我们提出了Transformer，一种避免循环并完全依赖注意力机制来绘制输入和输出之间全局依赖关系的模型架构。Transformer允许显著更多的并行化，并且在8个P100 GPU上训练短短12小时后就能在翻译质量上达到新的最先进水平。

# 2 背景

减少序列计算的目标也构成了Extended Neural GPU [16]、ByteNet [18]和ConvS2S [9]的基础，它们都使用卷积神经网络作为基本构建块，为所有输入和输出位置并行计算隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随位置间距离增长，ConvS2S是线性的，ByteNet是对数的。这使得学习远距离位置之间的依赖关系变得更加困难[12]。在Transformer中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而导致有效分辨率降低，我们在第3.2节中通过多头注意力来抵消这种影响。

自注意力，有时称为内部注意力，是一种通过关联单个序列的不同位置来计算序列表示的注意力机制。自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴含和学习任务无关的句子表示[4, 27, 28, 22]。

端到端记忆网络基于循环注意力机制而不是序列对齐的循环，并且已被证明在简单语言问答和语言建模任务上表现良好[34]。

据我们所知，Transformer是第一个完全依赖自注意力来计算其输入和输出表示而不使用序列对齐RNN或卷积的转换模型。在以下章节中，我们将描述Transformer，激发自注意力并讨论其相对于[17, 18]和[9]等模型的优势。

# 3 模型架构

大多数竞争性神经序列转换模型都具有编码器-解码器结构[5, 2, 35]。这里，编码器将符号表示的输入序列$(x_1, ..., x_n)$映射为连续表示序列$\mathbf{z} = (z_1, ..., z_n)$。给定$\mathbf{z}$，解码器然后逐个元素地生成符号输出序列$(y_1, ..., y_m)$。在每一步，模型都是自回归的[10]，在生成下一个符号时将先前生成的符号作为额外输入。

![](https://cdn-mineru.openxlab.org.cn/extract/95297a25-b897-45e1-b733-9db99ad6c2a8/0239f83d5c481c50892f873f6dc620cb886d8de06e7e11203dfead0f7e1765c9.jpg)  
图1：Transformer - 模型架构。

Transformer遵循这种整体架构，对编码器和解码器都使用堆叠的自注意力和逐点全连接层，分别如图1的左半部分和右半部分所示。

# 3.1 编码器和解码器堆栈

编码器：编码器由$N = 6$个相同层的堆栈组成。每层有两个子层。第一个是多头自注意力机制，第二个是简单的位置前馈全连接网络。我们在两个子层周围都采用残差连接[11]，然后进行层归一化[1]。也就是说，每个子层的输出是LayerNorm$(x + \mathrm{Sublayer}(x))$，其中Sublayer$(x)$是子层本身实现的函数。为了促进这些残差连接，模型中的所有子层以及嵌入层都产生维度为$d_{\mathrm{model}} = 512$的输出。

解码器：解码器也由$N = 6$个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器插入第三个子层，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。我们还修改解码器堆栈中的自注意力子层，以防止位置关注后续位置。这种掩蔽，结合输出嵌入偏移一个位置的事实，确保位置$i$的预测只能依赖于位置小于$i$的已知输出。

# 3.2 注意力

注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出计算为值的加权和

![](https://cdn-mineru.openxlab.org.cn/extract/95297a25-b897-45e1-b733-9db99ad6c2a8/eff597571c859ed52508605118e9276e94ce8bd8741cb887f344d4f2c3e12f45.jpg)  
图2：（左）缩放点积注意力。（右）多头注意力由几个并行运行的注意力层组成。

其中分配给每个值的权重由查询与相应键的兼容性函数计算。

# 3.2.1 缩放点积注意力

我们称我们的特定注意力为"缩放点积注意力"（图2）。输入包括维度为$d_k$的查询和键，以及维度为$d_v$的值。我们计算查询与所有键的点积，每个都除以$\sqrt{d_k}$，并应用softmax函数来获得值的权重。

在实践中，我们同时在一组查询上计算注意力函数，将它们打包成矩阵$Q$。键和值也打包成矩阵$K$和$V$。我们计算输出矩阵为：

$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

两个最常用的注意力函数是加性注意力[2]和点积（乘性）注意力。点积注意力与我们的算法相同，除了缩放因子$\frac{1}{\sqrt{d_k}}$。加性注意力使用具有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论复杂度上相似，但点积注意力在实践中更快且更节省空间，因为它可以使用高度优化的矩阵乘法代码实现。

虽然对于较小的$d_k$值，两种机制表现相似，但对于较大的$d_k$值，加性注意力优于无缩放的点积注意力[3]。我们怀疑对于较大的$d_k$值，点积在幅度上变大，将softmax函数推入梯度极小的区域4。为了抵消这种影响，我们将点积缩放$\frac{1}{\sqrt{d_k}}$。

# 3.2.2 多头注意力

我们发现，与使用$d_{\mathrm{model}}$维键、值和查询执行单个注意力函数相比，将查询、键和值分别用不同的学习线性投影$h$次线性投影到$d_k$、$d_k$和$d_v$维是有益的。在这些投影版本的查询、键和值上，我们然后并行执行注意力函数，产生$d_v$维输出值。这些被连接并再次投影，产生最终值，如图2所示。

多头注意力允许模型在不同位置共同关注来自不同表示子空间的信息。使用单个注意力头，平均会抑制这一点。

$$
\begin{array}{rl}
\mathrm{MultiHead}(Q, K, V) &= \mathrm{Concat}(\mathrm{head}_1, ..., \mathrm{head}_h)W^O \\
\text{其中 } \mathrm{head}_i &= \mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{array}
$$

其中投影是参数矩阵$W_i^Q \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$、$W_i^K \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$、$W_i^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_v}$和$W^O \in \mathbb{R}^{hd_v \times d_{\mathrm{model}}}$。

在这项工作中，我们使用$h = 8$个并行注意力层或头。对于每个，我们使用$d_k = d_v = d_{\mathrm{model}}/h = 64$。由于每个头的维度减少，总计算成本与具有完整维度的单头注意力相似。

# 3.2.3 注意力在我们模型中的应用

Transformer以三种不同方式使用多头注意力：

• 在"编码器-解码器注意力"层中，查询来自前一个解码器层，记忆键和值来自编码器的输出。这允许解码器中的每个位置关注输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如[38, 2, 9]。  
• 编码器包含自注意力层。在自注意力层中，所有键、值和查询都来自同一个地方，在这种情况下，是编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层中的所有位置。  
• 类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的向左信息流以保持自回归特性。我们通过在缩放点积注意力内部掩蔽（设置为$-\infty$）softmax输入中对应于非法连接的所有值来实现这一点。见图2。

# 3.3 位置前馈网络

除了注意力子层，我们的编码器和解码器中的每一层都包含一个全连接前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性变换，中间有一个ReLU激活。

$$
\mathrm{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

虽然线性变换在不同位置上是相同的，但它们在不同层之间使用不同的参数。另一种描述方式是两个核大小为1的卷积。输入和输出的维度是$d_{\mathrm{model}} = 512$，内层的维度是$d_{ff} = 2048$。

# 3.4 嵌入和Softmax

与其他序列转换模型类似，我们使用学习嵌入将输入标记和输出标记转换为维度为$d_{\mathrm{model}}$的向量。我们还使用常见的学习线性变换和softmax函数将解码器输出转换为预测的下一个标记概率。在我们的模型中，我们在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以$\sqrt{d_{\mathrm{model}}}$。

表1：不同层类型的最大路径长度、每层复杂度和最少序列操作数。$n$是序列长度，$d$是表示维度，$k$是卷积的核大小，$r$是受限自注意力中邻域的大小。

<html><body><table><tr><td>层类型</td><td>每层复杂度</td><td>序列操作</td><td>最大路径长度</td></tr><tr><td>自注意力</td><td>O(n² ·d)</td><td>0(1)</td><td>0(1)</td></tr><tr><td>循环</td><td>O(n ·d²)</td><td>O(n)</td><td>0(n）</td></tr><tr><td>卷积</td><td>O(k ·n · d²)</td><td>0(1)</td><td>O(logk(n))</td></tr><tr><td>自注意力（受限）</td><td>O(r ·n· d)</td><td>0(1)</td><td>O(n/r)</td></tr></table></body></html>

# 3.5 位置编码

由于我们的模型不包含循环和卷积，为了使模型利用序列的顺序，我们必须注入一些关于标记在序列中相对或绝对位置的信息。为此，我们在编码器和解码器堆栈底部的输入嵌入中添加"位置编码"。位置编码具有与嵌入相同的维度$d_{\mathrm{model}}$，以便可以将两者相加。有许多位置编码的选择，学习的和固定的[9]。

在这项工作中，我们使用不同频率的正弦和余弦函数：

$$
\begin{array}{r}
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{\mathrm{model}}}) \\
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{\mathrm{model}}})
\end{array}
$$

其中pos是位置，$i$是维度。也就是说，位置编码的每个维度对应一个正弦波。波长形成从$2\pi$到$10000 \cdot 2\pi$的几何级数。我们选择这个函数是因为我们假设它将允许模型轻松学习通过相对位置关注，因为对于任何固定偏移$k$，$PE_{pos+k}$可以表示为$PE_{pos}$的线性函数。

我们还尝试使用学习的位置嵌入[9]，发现两个版本产生几乎相同的结果（见表3行(E)）。我们选择正弦版本，因为它可能允许模型外推到比训练期间遇到的序列长度更长的序列长度。

# 4 为什么选择自注意力

在本节中，我们比较自注意力层与通常用于将一个可变长度符号表示序列$(x_1, ..., x_n)$映射到另一个等长序列$(z_1, ..., z_n)$的循环和卷积层的各个方面，其中$x_i, z_i \in \mathbb{R}^d$，例如典型序列转换编码器或解码器中的隐藏层。激发我们使用自注意力，我们考虑三个需求。

一个是每层的总计算复杂度。另一个是可以并行化的计算量，通过所需的最少序列操作数来衡量。

第三个是网络中长程依赖关系之间的路径长度。学习长程依赖关系是许多序列转换任务中的关键挑战。影响学习此类依赖关系能力的一个关键因素是前向和后向信号在网络中必须遍历的路径长度。输入和输出序列中任意位置组合之间的这些路径越短，就越容易学习长程依赖关系[12]。因此，我们还比较由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意力层以恒定数量的顺序执行操作连接所有位置，而循环层需要$O(n)$个顺序操作。在计算复杂度方面，当序列长度$n$小于表示维度$d$时，自注意力层比循环层更快，这在机器翻译中最先进模型使用的句子表示中最常见，如word-piece [38]和byte-pair [31]表示。为了改善涉及非常长序列的任务的计算性能，自注意力可以限制为仅考虑以相应输出位置为中心的输入序列中大小为$r$的邻域。这将最大路径长度增加到$O(n/r)$。我们计划在未来的工作中进一步研究这种方法。

核宽度$k < n$的单个卷积层不连接所有输入和输出位置对。在连续核的情况下需要$O(n/k)$个卷积层的堆栈，或在扩张卷积[18]的情况下需要$O(\log_k(n))$个，增加网络中任意两个位置之间最长路径的长度。卷积层通常比循环层更昂贵，因子为$k$。然而，可分离卷积[6]大大降低了复杂度，到$O(k \cdot n \cdot d + n \cdot d^2)$。即使$k = n$，可分离卷积的复杂度也等于自注意力层和逐点前馈层的组合，这是我们模型中采用的方法。

作为副作用，自注意力可以产生更可解释的模型。我们检查来自我们模型的注意力分布，并在附录中展示和讨论示例。不仅个别注意力头清楚地学会执行不同的任务，许多似乎表现出与句子的句法和语义结构相关的行为。

# 5 训练

本节描述我们模型的训练机制。

# 5.1 训练数据和批处理

我们在标准WMT 2014英德数据集上进行训练，该数据集包含约450万个句子对。句子使用字节对编码[3]进行编码，其共享源-目标词汇约37000个标记。对于英法，我们使用显著更大的WMT 2014英法数据集，包含3600万个句子，并将标记分割为32000个word-piece词汇[38]。句子对按近似序列长度批处理在一起。每个训练批次包含一组句子对，包含约25000个源标记和25000个目标标记。

# 5.2 硬件和时间安排

我们在一台配有8个NVIDIA P100 GPU的机器上训练我们的模型。对于使用整篇论文中描述的超参数的基础模型，每个训练步骤约需0.4秒。我们训练基础模型总共100,000步或12小时。对于我们的大模型（在表3底行描述），步骤时间为1.0秒。大模型训练了300,000步（3.5天）。

# 5.3 优化器

我们使用Adam优化器[20]，$\beta_1 = 0.9$，$\beta_2 = 0.98$和$\epsilon = 10^{-9}$。我们在训练过程中根据以下公式改变学习率：

$$
lrate = d_{\mathrm{model}}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})
$$

这对应于在前warmup_steps训练步骤中线性增加学习率，然后按步骤数的平方根倒数成比例地减少学习率。我们使用$warmup\_steps = 4000$。

# 5.4 正则化

我们在训练期间采用三种类型的正则化：

表2：Transformer在英德和英法newstest2014测试中以训练成本的一小部分获得了比先前最先进模型更好的BLEU分数。

<html><body><table><tr><td rowspan="2">模型</td><td colspan="2">BLEU</td><td colspan="2">训练成本 (FLOPs)</td></tr><tr><td>EN-DE</td><td>EN-FR</td><td>EN-DE</td><td>EN-FR</td></tr><tr><td>ByteNet [18]</td><td>23.75</td><td></td><td></td><td></td></tr><tr><td>Deep-Att+PosUnk[39]</td><td></td><td>39.2</td><td></td><td>1.0·1020</td></tr><tr><td>GNMT +RL[38]</td><td>24.6</td><td>39.92</td><td>2.3·1019</td><td>1.4 · 1020</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>9.6 · 1018</td><td>1.5 : 1020</td></tr><tr><td>MoE [32]</td><td>26.03</td><td>40.56</td><td>2.0·1019</td><td>1.2 · 1020</td></tr><tr><td>Deep-Att+PosUnk Ensemble [39]</td><td></td><td>40.4</td><td></td><td>8.0:1020</td></tr><tr><td>GNMT+RL Ensemble [38]</td><td>26.30</td><td>41.16</td><td>1.8·1020</td><td>1.1 · 1021</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>26.36</td><td>41.29</td><td>7.7· 1019</td><td>1.2 · 1021</td></tr><tr><td>Transformer (基础模型)</td><td>27.3</td><td>38.1</td><td>3.3·1018</td><td></td></tr><tr><td>Transformer (大模型)</td><td>28.4</td><td>41.8</td><td>2.3 : 1019</td><td></td></tr></table></body></html>

残差Dropout 我们对每个子层的输出应用dropout [33]，然后将其添加到子层输入并归一化。此外，我们对编码器和解码器堆栈中嵌入和位置编码的和应用dropout。对于基础模型，我们使用$P_{drop} = 0.1$的比率。

标签平滑 在训练期间，我们采用值为$\epsilon_{ls} = 0.1$的标签平滑[36]。这会损害困惑度，因为模型学会更加不确定，但提高了准确性和BLEU分数。

# 6 结果

# 6.1 机器翻译

在WMT 2014英德翻译任务上，大型transformer模型（表2中的Transformer (大模型)）比先前报告的最佳模型（包括集成模型）高出超过2.0 BLEU，建立了新的最先进BLEU分数28.4。该模型的配置列在表3的底行。训练在8个P100 GPU上花费了3.5天。即使我们的基础模型也超过了所有先前发布的模型和集成模型，训练成本只是任何竞争模型的一小部分。

在WMT 2014英法翻译任务上，我们的大模型达到了41.0的BLEU分数，超过了所有先前发布的单模型，训练成本不到先前最先进模型的1/4。用于英法训练的Transformer（大模型）使用dropout率$P_{drop} = 0.1$，而不是0.3。

对于基础模型，我们使用通过平均最后5个检查点获得的单个模型，这些检查点以10分钟间隔写入。对于大模型，我们平均最后20个检查点。我们使用束搜索，束大小为4，长度惩罚$\alpha = 0.6$ [38]。这些超参数是在开发集上实验后选择的。我们在推理期间将最大输出长度设置为输入长度$+ 50$，但在可能时提前终止[38]。

表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行比较。我们通过将训练时间、使用的GPU数量和每个GPU的持续单精度浮点容量的估计相乘来估计用于训练模型的浮点运算次数5。

# 6.2 模型变化

为了评估Transformer不同组件的重要性，我们以不同方式改变我们的基础模型，在开发集newstest2013上测量英德翻译性能的变化。我们使用前一节中描述的束搜索，但没有检查点平均。我们在表3中展示这些结果。

表3：Transformer架构的变化。未列出的值与基础模型相同。所有指标都在英德翻译开发集newstest2013上。列出的困惑度是每个word-piece，根据我们的字节对编码，不应与每个单词的困惑度进行比较。

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">N dmodel</td><td rowspan="2">dff</td><td rowspan="2">h</td><td rowspan="2">dk</td><td rowspan="2">d</td><td rowspan="2">Pdrop</td><td rowspan="2">es</td><td rowspan="2">trais</td><td rowspan="2">PPL)</td><td rowspan="2">BLEU</td><td rowspan="2">pxrams</td></tr><tr><td></td></tr><tr><td>基础</td><td>6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>64</td><td>0.1 0.1</td><td>100K</td><td>4.92</td><td></td><td>25.8</td><td>65</td></tr><tr><td rowspan="3">(A)</td><td></td><td></td><td>1</td><td>512</td><td>512</td><td></td><td></td><td></td><td>5.29</td><td></td><td>24.9</td><td></td></tr><tr><td></td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td>16 32</td><td>32 16</td><td>32 16</td><td></td><td></td><td></td><td>4.91 5.01</td><td></td><td>25.8</td><td></td></tr><tr><td>(B)</td><td></td><td></td><td></td><td></td><td>16 32</td><td></td><td></td><td></td><td></td><td>5.16 5.01</td><td>25.4 25.1 25.4</td><td>58</td></tr><tr><td rowspan="4">(C)</td><td>2 4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td></td><td>23.7</td><td>60 36</td></tr><tr><td rowspan="4">8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td>256</td><td></td><td></td><td>32</td><td>32</td><td></td><td></td><td></td><td>4.88 5.75</td><td>25.5 24.5</td><td>80</td></tr><tr><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>28 168</td></tr><tr><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td rowspan="3">(D)</td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.75</td><td></td><td>26.2</td><td>90</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>5.77</td><td>24.6</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>4.95</td><td>25.5</td><td></td><td></td></tr><tr><td rowspan="2">(E)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0 0.2</td><td></td><td>4.67</td><td>25.3</td><td></td></tr><tr><td colspan="4">位置嵌入代替正弦</td><td></td><td></td><td></td><td></td><td>5.47 4.92</td><td>25.7</td><td></td></tr><tr><td>大模型</td><td>6</td><td>1024 4096</td><td></td><td>16</td><td></td><td></td><td>0.3</td><td>300K</td><td></td><td>4.33</td><td>25.7 26.4</td><td>213</td></tr></table></body></html>

在表3行(A)中，我们改变注意力头数和注意力键值维度，保持计算量恒定，如第3.2.2节所述。虽然单头注意力比最佳设置差0.9 BLEU，但头数过多时质量也会下降。

在表3行(B)中，我们观察到减少注意力键大小$d_k$会损害模型质量。这表明确定兼容性并不容易，比点积更复杂的兼容性函数可能是有益的。我们在行(C)和(D)中进一步观察到，如预期的那样，更大的模型更好，dropout对避免过拟合非常有帮助。在行(E)中，我们用学习的位置嵌入[9]替换我们的正弦位置编码，观察到与基础模型几乎相同的结果。

# 6.3 英语成分句法分析

为了评估Transformer是否可以泛化到其他任务，我们对英语成分句法分析进行了实验。这个任务提出了特定的挑战：输出受到强结构约束，且明显长于输入。此外，RNN序列到序列模型在小数据机制中无法达到最先进的结果[37]。

我们在Penn Treebank [25]的华尔街日报(WSJ)部分训练了一个4层transformer，$d_{model} = 1024$，约40K训练句子。我们还在半监督设置中训练它，使用来自的更大的高置信度和BerkleyParser语料库，约17M句子[37]。我们对仅WSJ设置使用16K标记词汇，对半监督设置使用32K标记词汇。

我们只进行了少量实验来在第22节开发集上选择dropout、注意力和残差（第5.4节）、学习率和束大小，所有其他参数保持与英德基础翻译模型相同。在推理期间，我们将最大输出长度增加到输入长度$+ 300$。我们对仅WSJ和半监督设置都使用束大小21和$\alpha = 0.3$。

表4：Transformer在英语成分句法分析上泛化良好（结果在WSJ的第23节）

<html><body><table><tr><td>解析器</td><td>训练</td><td>WSJ 23F1</td></tr><tr><td>Vinyals&Kaiserelal. (2014)[37]</td><td>仅WSJ，判别式 仅WSJ，判别式</td><td>88.3 90.4</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>仅WSJ，判别式</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) [8] Transformer (4层)</td><td>仅WSJ，判别式 仅WSJ，判别式</td><td>91.7</td></tr><tr><td></td><td></td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>半监督</td><td>91.3</td></tr><tr><td>Huang&Harper (2009)[14]</td><td>半监督</td><td>91.3</td></tr><tr><td>McClosky et al. (2006)[26]</td><td>半监督</td><td>92.1</td></tr><tr><td>Vinyals&Kaiser elal. (2014)[37]</td><td>半监督</td><td>92.1</td></tr><tr><td>Transformer (4层)</td><td>半监督</td><td>92.7</td></tr><tr><td>Luong et al. (2015) [23]</td><td>多任务</td><td>93.0</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>生成式</td><td>93.3</td></tr></table></body></html>

表4中的结果显示，尽管缺乏任务特定的调优，我们的模型表现出色，产生了比除循环神经网络语法[8]外所有先前报告模型更好的结果。

与RNN序列到序列模型[37]相比，即使仅在40K句子的WSJ训练集上训练，Transformer也超过了BerkeleyParser [29]。

# 7 结论

在这项工作中，我们提出了Transformer，第一个完全基于注意力的序列转换模型，用多头自注意力替换编码器-解码器架构中最常用的循环层。

对于翻译任务，Transformer可以比基于循环或卷积层的架构训练得显著更快。在WMT 2014英德和WMT 2014英法翻译任务上，我们都达到了新的最先进水平。在前一个任务中，我们的最佳模型甚至超过了所有先前报告的集成模型。

我们对基于注意力的模型的未来感到兴奋，并计划将它们应用于其他任务。我们计划将Transformer扩展到涉及文本以外的输入和输出模态的问题，并研究局部、受限的注意力机制以有效处理大型输入和输出，如图像、音频和视频。使生成不那么顺序化是我们的另一个研究目标。

我们用于训练和评估模型的代码可在https://github.com/tensorflow/tensor2tensor获得。

致谢 我们感谢Nal Kalchbrenner和Stephan Gouws的富有成果的评论、更正和启发。 